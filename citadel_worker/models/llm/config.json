{
  "name": "llm",
  "streaming": true,
  "inputs": [
    { "name": "text", "type": "str" },
    { "name": "temperature", "type": "float" },
    { "name": "max_tokens", "type": "int" },
    { "name": "top_p", "type": "float" },
    { "name": "top_k", "type": "int" }
  ],
  "output": { "name": "answer", "type": "str" },
  "version": "0.1.0",
  "description": "Query the LLM with the given text.",
  "conda_env": "C:\\citadel\\models\\llm_env_win",
  "model_path": "C:\\citadel\\models\\llama8b_awq",
  "gpu_memory": 0.5,
  "gpu_ids": [1],
  "model_per_gpu": 1,
  "timeout": 900000,
  "concurrency": 1,
  "score_weights": {
    "lat": 1.0,
    "queue": 0.5
  },
  "warmup": [
    {
      "input": {
        "text": "Hello, how are you?",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 1.0,
        "top_k": 50
      },
      "amount": 3
    }
  ]
}
